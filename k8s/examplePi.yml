---

apiVersion: batch/v1
kind: Job
metadata:
  name: submit
  namespace: spark-submit
  labels:
    app.kubernetes.io/name: spark-submit
    spark-role: launcher
spec:
  template:
    spec:
      serviceAccountName: spark-submit
      restartPolicy: Never
      containers:
      - name: test
        image: gcr.io/spark-operator/spark:v2.4.5
        command: [ "/opt/spark/bin/spark-submit" ]
        args: [ "--master", "k8s://https://kubernetes.default", 
                "--deploy-mode", "cluster",
                "--name", "spark-pi",
                "--conf", "spark.executor.instances=2",
                # Kubrnetes authorization config
                "--conf", "spark.kubernetes.namespace=spark-submit",
                "--conf", "spark.kubernetes.authenticate.driver.serviceAccountName=spark-submit",
                #"--conf", "spark.kubernetes.node.selector.*",
                "--conf", "spark.kubernetes.container.image=luismiguelsaez/spark-operator:custom-v2.4.5",
                # Pods labeling
                "--conf", "spark.kubernetes.driver.label.spark-commit-role=driver",
                "--conf", "spark.kubernetes.executor.label.spark-commit-role=executor",
                # Driver resource allocation
                "--conf", "spark.kubernetes.driver.request.cores=1",
                "--conf", "spark.kubernetes.driver.limit.cores=1",
                "--conf", "spark.kubernetes.driver.request.memory=256m",
                "--conf", "spark.kubernetes.driver.limit.memory=256m",
                # Executor resource allocation
                "--conf", "spark.kubernetes.executor.request.cores=1",
                "--conf", "spark.kubernetes.executor.limit.cores=1",
                "--conf", "spark.kubernetes.executor.request.memory=256m",
                "--conf", "spark.kubernetes.executor.limit.memory=1g",
                # Driver/executor shared volume
                "--conf", "spark.kubernetes.volumes.emptyDir.spark-shared.mount.path=/tmp",
                # Delete executor pods on termination
                "--conf", "spark.kubernetes.executor.deleteOnTermination=false",
                # Path for side upload in cluster mode
                "--conf", "spark.kubernetes.file.upload.path=/tmp/sideupload",
                # Extra options
                "--conf", "spark.driver.extraJavaOptions=-Divy.cache.dir=/tmp -Divy.home=/tmp",
                # Extra packages to upload to executors
                #"--packages", "com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.6",
                # Extra classpath packages for driver
                #"--conf", "spark.executor.extraClassPath=/opt/spark/jars/hadoop-client-2.7.3.jar:/opt/spark/jars/hadoop-common-3.3.0.jar",
                # Additional dependencies
                "--jars", "/opt/spark/jars/hadoop-client-2.7.3.jar,/opt/spark/jars/hadoop-common-3.3.0.jar",
                # Package class to launch
                "--class", "org.apache.spark.examples.SparkPi",
                # Application package to launch
                "local:///opt/spark/examples/jars/spark-examples_2.11-2.4.5.jar" ]
